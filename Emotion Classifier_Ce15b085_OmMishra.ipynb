{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Emotion Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset set contained in four text files consists of tweets for four different emotions: anger, fear, joy and sadness.<br>\n",
    "\n",
    "Along with the tweet, the intensity or degree of emotion X felt by the speaker (a real-valued score between 0 and 1) is also provided. <br>\n",
    "\n",
    "The maximum possible score 1 stands for feeling the maximum amount of emotion X (or having a mental state maximally inclined towards feeling emotion X). The minimum possible score 0 stands for feeling the least amount of emotion X (or having a mental state maximally away from feeling emotion X). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Goals: \n",
    "i) To classify a given tweet into one of the four classes: anger, fear, joy or sadness. <br>\n",
    "ii) To display the degree of the classified emotion in the tweet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Installing required package:<br>\n",
    "```\n",
    "pip3 install nltk\n",
    " (or)\n",
    "pip install nltk\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.pipeline import make_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>tweet</th>\n",
       "      <th>emotion</th>\n",
       "      <th>intensity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10000</td>\n",
       "      <td>How the fu*k! Who the heck! moved my fridge!... should I knock the landlord door. #angry #mad ##</td>\n",
       "      <td>anger</td>\n",
       "      <td>0.938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10001</td>\n",
       "      <td>So my Indian Uber driver just called someone the N word. If I wasn't in a moving vehicle I'd have jumped out #disgusted</td>\n",
       "      <td>anger</td>\n",
       "      <td>0.896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10002</td>\n",
       "      <td>@DPD_UK I asked for my parcel to be delivered to a pick up store not my address #fuming #poorcustomerservice</td>\n",
       "      <td>anger</td>\n",
       "      <td>0.896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10003</td>\n",
       "      <td>so ef whichever butt wipe pulled the fire alarm in davis bc I was sound asleep #pissed #angry #upset #tired #sad #tired #hangry ######</td>\n",
       "      <td>anger</td>\n",
       "      <td>0.896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10004</td>\n",
       "      <td>Don't join @BTCare they put the phone down on you, talk over you and are rude. Taking money out of my acc willynilly! #fuming</td>\n",
       "      <td>anger</td>\n",
       "      <td>0.896</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id  \\\n",
       "0  10000   \n",
       "1  10001   \n",
       "2  10002   \n",
       "3  10003   \n",
       "4  10004   \n",
       "\n",
       "                                                                                                                                    tweet  \\\n",
       "0  How the fu*k! Who the heck! moved my fridge!... should I knock the landlord door. #angry #mad ##                                         \n",
       "1  So my Indian Uber driver just called someone the N word. If I wasn't in a moving vehicle I'd have jumped out #disgusted                  \n",
       "2  @DPD_UK I asked for my parcel to be delivered to a pick up store not my address #fuming #poorcustomerservice                             \n",
       "3  so ef whichever butt wipe pulled the fire alarm in davis bc I was sound asleep #pissed #angry #upset #tired #sad #tired #hangry ######   \n",
       "4  Don't join @BTCare they put the phone down on you, talk over you and are rude. Taking money out of my acc willynilly! #fuming            \n",
       "\n",
       "  emotion  intensity  \n",
       "0  anger   0.938      \n",
       "1  anger   0.896      \n",
       "2  anger   0.896      \n",
       "3  anger   0.896      \n",
       "4  anger   0.896      "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pandas import DataFrame\n",
    "import pandas as pd\n",
    "\n",
    "data = [] # Tweets\n",
    "data_labels = [] # Emotion label (anger, fear, joy, or sadness)\n",
    "data_int = [] # Intensityy of each emotion\n",
    "\n",
    "dataset=pd.read_csv(\"training_set/anger-ratings-0to1.train.txt\",delimiter=\"\\t\",names=['id','tweet','emotion','intensity'])\n",
    "\n",
    "# Display first few examples\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "857"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reading the tweets and their corresponding emotion and intensity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import DataFrame\n",
    "import pandas as pd\n",
    "\n",
    "data = [] # Tweets\n",
    "data_labels = [] # Emotion label (anger, fear, joy, or sadness)\n",
    "data_int = [] # Intensityy of each emotion\n",
    "\n",
    "dataset=pd.read_csv(\"training_set/anger-ratings-0to1.train.txt\",delimiter=\"\\t\",names=['id','tweet','emotion','intensity'])\n",
    "for i in range(len(dataset)):\n",
    "    data.append(dataset.iat[i,1])\n",
    "    data_labels.append('anger')\n",
    "    data_int.append(dataset.iat[i,3])\n",
    "    \n",
    "dataset=pd.read_csv(\"training_set/fear-ratings-0to1.train.txt\",delimiter=\"\\t\",names=['id','tweet','emotion','intensity'])\n",
    "for i in range(len(dataset)):\n",
    "    data.append(dataset.iat[i,1])\n",
    "    data_labels.append('fear')\n",
    "    data_int.append(dataset.iat[i,3])\n",
    "\n",
    "dataset=pd.read_csv(\"training_set/joy-ratings-0to1.train.txt\",delimiter=\"\\t\",names=['id','tweet','emotion','intensity'])\n",
    "for i in range(len(dataset)):\n",
    "    data.append(dataset.iat[i,1])\n",
    "    data_labels.append('joy')\n",
    "    data_int.append(dataset.iat[i,3])\n",
    "\n",
    "dataset=pd.read_csv(\"training_set/sadness-ratings-0to1.train.txt\",delimiter=\"\\t\",names=['id','tweet','emotion','intensity'])\n",
    "for i in range(len(dataset)):\n",
    "    data.append(dataset.iat[i,1])\n",
    "    data_labels.append('sadness')\n",
    "    data_int.append(dataset.iat[i,3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Shuffling the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import shuffle\n",
    "dv = []\n",
    "dl = []\n",
    "di = []\n",
    "index_shuf = list(range(len(data)))\n",
    "shuffle(index_shuf)\n",
    "for i in index_shuf:\n",
    "    dv.append(data[i])\n",
    "    dl.append(data_labels[i])\n",
    "    di.append(data_int[i])\n",
    "data = dv\n",
    "data_labels = dl\n",
    "data_int = di"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature extraction using CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer    \n",
    "\n",
    "vectorizer = CountVectorizer(\n",
    "    analyzer = 'word',\n",
    "    lowercase = False,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### An example using CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['this is great', 'This is too great to be great', 'THIS IS GREAT!']\n"
     ]
    }
   ],
   "source": [
    "example = ['this is great','This is too great to be great','THIS IS GREAT!']\n",
    "print(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['GREAT', 'IS', 'THIS', 'This', 'be', 'great', 'is', 'this', 'to', 'too']\n",
      "[[0 0 0 0 0 1 1 1 0 0]\n",
      " [0 0 0 1 1 2 1 0 1 1]\n",
      " [1 1 1 0 0 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "features_eg = vectorizer.fit_transform(\n",
    "    example\n",
    ")\n",
    "features_nd_eg = features_eg.toarray() # for easy usage\n",
    "print(vectorizer.get_feature_names())\n",
    "print(features_nd_eg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extracting features from tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = vectorizer.fit_transform(\n",
    "    data\n",
    ")\n",
    "features_nd = features.toarray() # for easy usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3613, 11239)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_nd.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test  = train_test_split(\n",
    "        features_nd, \n",
    "        data_labels,\n",
    "        train_size=0.80, test_size=0.20, \n",
    "        random_state=1234)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "log_model = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_model = log_model.fit(X=X_train, y=y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = log_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8561549100968188"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.mean(y_pred==y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sadness : @ticcikasie1 With a frown, she let's out a distraught 'Gardevoir' saying that she wishes she had a trainer\n",
      "fear : What an actual nightmare\n",
      "anger : It takes a man to suffer ignorance and smile. Be yourself, no matter what they say. #sting\n",
      "anger : ordered my vacation bathing suits. one less thing to fret about.\n",
      "anger : Get to work and there's a fire drill. #fire  #outthere #inthedark\n",
      "fear : Rojo is shocking.......absolutely shocking !!!\n",
      "fear : STAY JADED everyone is #terrible\n"
     ]
    }
   ],
   "source": [
    "# Printing the predictions for some random test data\n",
    "import random\n",
    "\n",
    "j = random.randint(0,len(X_test)-7)\n",
    "for i in range(j,j+7):\n",
    "    ind = features_nd.tolist().index(X_test[i].tolist())\n",
    "    print(y_pred[i],\":\",data[ind].strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8561549100968188\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "print(accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "```\n",
    "There are two sets each containing 4 files for each emotion provided for training and development. \n",
    "Combine these two sets for training and use 5-fold cross-validation \n",
    "to find out the Accuracy in all the cases mentioned below.\n",
    "```\n",
    "\n",
    "1. Calculate the accuracy using Random Forest Classifier and tune the number of estimators to get the best results. Comment on the same.\n",
    "2. Now use Logistic Regression and observe the accuracy value. Can the performance be further improved by using L1 and L2 regularizations?\n",
    "3. Repeat the same using Support Vector Classifier.\n",
    "4. Estimate the training & testing time for each classifier and comment on the results.\n",
    "5. Now, the emotion intensity score for each tweet is to be found on top of classification. To do this, fit different regression models on the training set for each emotion and find the emotion intensity score for each of the test set. Also, display mean square error for test set.\n",
    "6. In all the above cases, create a user-defined function, which takes a tweet (text) as input and displays the predicted emotion.\n",
    "7. A separate test set is provided. Use one of the classification models implemented earlier to determine the corresponding emotion for each tweet in this set. Use the linear regression models to calculate the emotional intensity.\n",
    "\n",
    "```In all the above cases, use a feature extractor other than CountVectorizer and observe performance.```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3613, 11239)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_nd.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data extracting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import DataFrame\n",
    "import pandas as pd\n",
    "\n",
    "data = [] # Tweets\n",
    "data_labels = [] # Emotion label (anger, fear, joy, or sadness)\n",
    "data_int = [] # Intensityy of each emotion\n",
    "\n",
    "dataset=pd.read_csv(\"training_set/anger-ratings-0to1.train.txt\",delimiter=\"\\t\",names=['id','tweet','emotion','intensity'])\n",
    "for i in range(len(dataset)):\n",
    "    data.append(dataset.iat[i,1])\n",
    "    data_labels.append('anger')\n",
    "    data_int.append(dataset.iat[i,3])\n",
    "    \n",
    "dataset=pd.read_csv(\"training_set/fear-ratings-0to1.train.txt\",delimiter=\"\\t\",names=['id','tweet','emotion','intensity'])\n",
    "for i in range(len(dataset)):\n",
    "    data.append(dataset.iat[i,1])\n",
    "    data_labels.append('fear')\n",
    "    data_int.append(dataset.iat[i,3])\n",
    "\n",
    "dataset=pd.read_csv(\"training_set/joy-ratings-0to1.train.txt\",delimiter=\"\\t\",names=['id','tweet','emotion','intensity'])\n",
    "for i in range(len(dataset)):\n",
    "    data.append(dataset.iat[i,1])\n",
    "    data_labels.append('joy')\n",
    "    data_int.append(dataset.iat[i,3])\n",
    "\n",
    "dataset=pd.read_csv(\"training_set/sadness-ratings-0to1.train.txt\",delimiter=\"\\t\",names=['id','tweet','emotion','intensity'])\n",
    "for i in range(len(dataset)):\n",
    "    data.append(dataset.iat[i,1])\n",
    "    data_labels.append('sadness')\n",
    "    data_int.append(dataset.iat[i,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset=pd.read_csv(\"dev_set/anger-ratings-0to1.dev.gold.txt\",delimiter=\"\\t\",names=['id','tweet','emotion','intensity'])\n",
    "for i in range(len(dataset)):\n",
    "    data.append(dataset.iat[i,1])\n",
    "    data_labels.append('anger')\n",
    "    data_int.append(dataset.iat[i,3])\n",
    "    \n",
    "dataset=pd.read_csv(\"dev_set/fear-ratings-0to1.dev.gold.txt\",delimiter=\"\\t\",names=['id','tweet','emotion','intensity'])\n",
    "for i in range(len(dataset)):\n",
    "    data.append(dataset.iat[i,1])\n",
    "    data_labels.append('fear')\n",
    "    data_int.append(dataset.iat[i,3])\n",
    "\n",
    "dataset=pd.read_csv(\"dev_set/joy-ratings-0to1.dev.gold.txt\",delimiter=\"\\t\",names=['id','tweet','emotion','intensity'])\n",
    "for i in range(len(dataset)):\n",
    "    data.append(dataset.iat[i,1])\n",
    "    data_labels.append('joy')\n",
    "    data_int.append(dataset.iat[i,3])\n",
    "\n",
    "dataset=pd.read_csv(\"dev_set/sadness-ratings-0to1.dev.gold.txt\",delimiter=\"\\t\",names=['id','tweet','emotion','intensity'])\n",
    "for i in range(len(dataset)):\n",
    "    data.append(dataset.iat[i,1])\n",
    "    data_labels.append('sadness')\n",
    "    data_int.append(dataset.iat[i,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import DataFrame\n",
    "import pandas as pd\n",
    "\n",
    "testdata = [] # Tweets\n",
    "testdata_labels = [] # Emotion label (anger, fear, joy, or sadness)\n",
    "testdata_int = [] # Intensityy of each emotion\n",
    "\n",
    "dataset=pd.read_csv(\"testing_set/anger-ratings-0to1.test.gold.txt\",delimiter=\"\\t\",names=['id','tweet','emotion','intensity'])\n",
    "for i in range(len(dataset)):\n",
    "    testdata.append(dataset.iat[i,1])\n",
    "    testdata_labels.append('anger')\n",
    "    testdata_int.append(dataset.iat[i,3])\n",
    "    \n",
    "dataset=pd.read_csv(\"testing_set/fear-ratings-0to1.test.gold.txt\",delimiter=\"\\t\",names=['id','tweet','emotion','intensity'])\n",
    "for i in range(len(dataset)):\n",
    "    testdata.append(dataset.iat[i,1])\n",
    "    testdata_labels.append('fear')\n",
    "    testdata_int.append(dataset.iat[i,3])\n",
    "\n",
    "dataset=pd.read_csv(\"testing_set/joy-ratings-0to1.test.gold.txt\",delimiter=\"\\t\",names=['id','tweet','emotion','intensity'])\n",
    "for i in range(len(dataset)):\n",
    "    testdata.append(dataset.iat[i,1])\n",
    "    testdata_labels.append('joy')\n",
    "    testdata_int.append(dataset.iat[i,3])\n",
    "\n",
    "dataset=pd.read_csv(\"testing_set/sadness-ratings-0to1.test.gold.txt\",delimiter=\"\\t\",names=['id','tweet','emotion','intensity'])\n",
    "for i in range(len(dataset)):\n",
    "    testdata.append(dataset.iat[i,1])\n",
    "    testdata_labels.append('sadness')\n",
    "    testdata_int.append(dataset.iat[i,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer    \n",
    "\n",
    "vectorizer = CountVectorizer(\n",
    "    analyzer = 'word',\n",
    "    lowercase = False,\n",
    ")\n",
    "features = vectorizer.fit_transform(\n",
    "    data\n",
    ")\n",
    "features_nd = features.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test  = train_test_split(\n",
    "        features_nd, \n",
    "        data_labels,\n",
    "        train_size=0.80, test_size=0.20, \n",
    "        random_state=1234)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time 757.3631913413255\n",
      "CV accuracy scores: [0.84591195 0.80757098 0.82306477 0.83728278 0.82911392]\n",
      "CV accuracy: 0.829 +/- 0.013\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score \n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "model=RandomForestClassifier(criterion='gini',n_estimators=500,min_samples_leaf=2)\n",
    "import timeit\n",
    "tic = timeit.default_timer()\n",
    "# X_train is split into training and testing sets for each of the k folds and scores are obtained\n",
    "scores = cross_val_score(estimator=model, X=X_train, y=y_train, cv=5, n_jobs=-1)\n",
    "toc = timeit.default_timer()\n",
    "print(\"Training time\" , toc - tic)\n",
    "print('CV accuracy scores: %s' % scores) \n",
    "print('CV accuracy: %.3f +/- %.3f' % (np.mean(scores), np.std(scores))) \n",
    "from sklearn.metrics import classification_report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\om mishra\\Anaconda3\\lib\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "C:\\Users\\om mishra\\Anaconda3\\lib\\site-packages\\sklearn\\grid_search.py:42: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 17min 17s\n",
      "{'n_estimators': 100}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.grid_search import GridSearchCV\n",
    "param_grid = {'n_estimators': [100,200,400,500,600]}\n",
    "grid = GridSearchCV(model, param_grid,n_jobs=-1)\n",
    "%time grid.fit(X_train, y_train)\n",
    "print(grid.best_params_)\n",
    "model = grid.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time 45.385237231955216\n",
      "Classification report for validation dataset\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      anger       0.94      0.75      0.83       197\n",
      "       fear       0.70      0.95      0.81       240\n",
      "        joy       0.96      0.83      0.89       191\n",
      "    sadness       0.88      0.77      0.82       164\n",
      "\n",
      "avg / total       0.86      0.83      0.84       792\n",
      "\n",
      "Testing time time 0.710537148117055\n"
     ]
    }
   ],
   "source": [
    "import timeit\n",
    "tic = timeit.default_timer()\n",
    "model.fit(X_train, y_train)\n",
    "toc = timeit.default_timer()\n",
    "print(\"Training time\" , toc - tic)\n",
    "from sklearn.metrics import classification_report\n",
    "tic = timeit.default_timer()\n",
    "y_fit=model.predict(X_test)\n",
    "print(\"Classification report for validation dataset\\n\",classification_report(y_test, y_fit))\n",
    "toc = timeit.default_timer()\n",
    "print(\"Testing time time\" , toc - tic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### If we use random forest the cross validation accuracy is 0.83. It is less than logistic regression and SVM(linear) because the boundary would be linear in nature and random forest is trying to make boxes but is unable to do a very good job"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time 1.9924491568635858\n",
      "Classification report for validation dataset\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      anger       0.88      0.86      0.87       197\n",
      "       fear       0.82      0.93      0.87       240\n",
      "        joy       0.94      0.85      0.89       191\n",
      "    sadness       0.84      0.80      0.82       164\n",
      "\n",
      "avg / total       0.87      0.87      0.87       792\n",
      "\n",
      "Testing time time 0.471371835539685\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "model=LogisticRegression()\n",
    "tic = timeit.default_timer()\n",
    "model.fit(X_train, y_train)\n",
    "toc = timeit.default_timer()\n",
    "print(\"Training time\" , toc - tic)\n",
    "tic = timeit.default_timer()\n",
    "y_fit=model.predict(X_test)\n",
    "print(\"Classification report for validation dataset\\n\",classification_report(y_test, y_fit))\n",
    "toc = timeit.default_timer()\n",
    "print(\"Testing time time\" , toc - tic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time 165.31165567550306\n",
      "CV accuracy scores: [0.83647799 0.8044164  0.835703   0.84202212 0.82594937]\n",
      "CV accuracy: 0.829 +/- 0.013\n"
     ]
    }
   ],
   "source": [
    "tic = timeit.default_timer()\n",
    "# X_train is split into training and testing sets for each of the k folds and scores are obtained\n",
    "scores = cross_val_score(estimator=model, X=X_train, y=y_train, cv=5, n_jobs=-1)\n",
    "toc = timeit.default_timer()\n",
    "print(\"Training time\" , toc - tic)\n",
    "print('CV accuracy scores: %s' % scores) \n",
    "print('CV accuracy: %.3f +/- %.3f' % (np.mean(scores), np.std(scores))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 6min 29s\n",
      "{'C': 5, 'penalty': 'l1'}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.grid_search import GridSearchCV\n",
    "param_grid = {'penalty': ['l1','l2'],\n",
    "             'C':[1,5,10]}\n",
    "grid = GridSearchCV(model, param_grid,n_jobs=-1)\n",
    "%time grid.fit(X_train, y_train)\n",
    "print(grid.best_params_)\n",
    "model = grid.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification report for validation dataset\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      anger       0.93      0.82      0.87       197\n",
      "       fear       0.78      0.93      0.85       240\n",
      "        joy       0.95      0.87      0.91       191\n",
      "    sadness       0.87      0.84      0.85       164\n",
      "\n",
      "avg / total       0.87      0.87      0.87       792\n",
      "\n",
      "Testing time time 2.659489818408474\n"
     ]
    }
   ],
   "source": [
    "tic = timeit.default_timer()\n",
    "y_fit=model.predict(X_test)\n",
    "print(\"Classification report for validation dataset\\n\",classification_report(y_test, y_fit))\n",
    "toc = timeit.default_timer()\n",
    "print(\"Testing time time\" , toc - tic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time 72.60058481490705\n",
      "CV accuracy scores: [0.8663522  0.83280757 0.83886256 0.84834123 0.83544304]\n",
      "CV accuracy: 0.844 +/- 0.012\n"
     ]
    }
   ],
   "source": [
    "tic = timeit.default_timer()\n",
    "# X_train is split into training and testing sets for each of the k folds and scores are obtained\n",
    "scores = cross_val_score(estimator=model, X=X_train, y=y_train, cv=5, n_jobs=-1)\n",
    "toc = timeit.default_timer()\n",
    "print(\"Training time\" , toc - tic)\n",
    "print('CV accuracy scores: %s' % scores) \n",
    "print('CV accuracy: %.3f +/- %.3f' % (np.mean(scores), np.std(scores))) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The logistic regression Cross validation accuracy increased once we used L1 regularization. This might be because of multicollinearity in the data and because of regularization we are able to mitigate its effect on accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "model=SVC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time 278.18751782643085\n"
     ]
    }
   ],
   "source": [
    "model=SVC(kernel='linear')\n",
    "tic = timeit.default_timer()\n",
    "model.fit(X_train, y_train)\n",
    "toc = timeit.default_timer()\n",
    "print(\"Training time\" , toc - tic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification report for validation dataset\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      anger       0.89      0.83      0.86       197\n",
      "       fear       0.79      0.95      0.86       240\n",
      "        joy       0.94      0.84      0.89       191\n",
      "    sadness       0.85      0.77      0.81       164\n",
      "\n",
      "avg / total       0.86      0.85      0.85       792\n",
      "\n",
      "Testing time time 43.52747052696941\n"
     ]
    }
   ],
   "source": [
    "tic = timeit.default_timer()\n",
    "y_fit=model.predict(X_test)\n",
    "print(\"Classification report for validation dataset\\n\",classification_report(y_test, y_fit))\n",
    "toc = timeit.default_timer()\n",
    "print(\"Testing time time\" , toc - tic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time 595.6886770845049\n",
      "CV accuracy scores: [0.83490566 0.78391167 0.82464455 0.81516588 0.8085443 ]\n",
      "CV accuracy: 0.813 +/- 0.017\n"
     ]
    }
   ],
   "source": [
    "tic = timeit.default_timer()\n",
    "# X_train is split into training and testing sets for each of the k folds and scores are obtained\n",
    "scores = cross_val_score(estimator=model, X=X_train, y=y_train, cv=5, n_jobs=-1)\n",
    "toc = timeit.default_timer()\n",
    "print(\"Training time\" , toc - tic)\n",
    "print('CV accuracy scores: %s' % scores) \n",
    "print('CV accuracy: %.3f +/- %.3f' % (np.mean(scores), np.std(scores))) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear kernel of SVM performed much better than Rbf because the boundary might be linear. The boundary being linear is also seen by higher accuracy of logistic regression than random forrest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and testing time is least for logistic regression. It is less than random forest because random forest has 500 eastimators and hence it will be more complex and take more time. SVM is not good with a lot of features and in this we have around 12000 features and hence it is taking a lot of time to classify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_fear=[]\n",
    "data_anger=[]\n",
    "data_joy=[]\n",
    "data_sadness=[]\n",
    "target_fear=[]\n",
    "target_anger=[]\n",
    "target_joy=[]\n",
    "target_sadness=[]\n",
    "for i in range(len(data_labels)):\n",
    "    if(data_labels[i]=='fear'):\n",
    "        data_fear.append(features_nd[i])\n",
    "        target_fear.append(data_int[i])\n",
    "    if(data_labels[i]=='anger'):\n",
    "        data_anger.append(features_nd[i])\n",
    "        target_anger.append(data_int[i])\n",
    "    if(data_labels[i]=='joy'):\n",
    "        data_joy.append(features_nd[i])\n",
    "        target_joy.append(data_int[i])\n",
    "    if(data_labels[i]=='sadness'):\n",
    "        data_sadness.append(features_nd[i])\n",
    "        target_sadness.append(data_int[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Anger intensity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "anger=RandomForestRegressor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 1 Mean Squared Error: 0.024\n",
      "Fold: 2 Mean Squared Error: 0.025\n",
      "Fold: 3 Mean Squared Error: 0.025\n",
      "Fold: 4 Mean Squared Error: 0.019\n",
      "Fold: 5 Mean Squared Error: 0.022\n"
     ]
    }
   ],
   "source": [
    "data_anger=np.array(data_anger)\n",
    "target_anger=np.array(target_anger)\n",
    "#anger=LinearRegression()\n",
    "anger=RandomForestRegressor()\n",
    "X_train, X_test, y_train, y_test = train_test_split(data_anger, target_anger, test_size=0.20, random_state=1234) \n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "# Split the data set into 'k' folds\n",
    "#kfold = StratifiedKFold(n_splits=5, random_state=1)\n",
    "kfold = KFold(n_splits=5, random_state=1)\n",
    "scores = [] \n",
    "k = 0\n",
    "\n",
    "for (train, test) in kfold.split(X_train, y_train): \n",
    "    anger.fit(X_train[train], y_train[train])          # Perform functions in pipeline\n",
    "    score = anger.score(X_train[test], y_train[test])  # Calculate score for each fold \n",
    "    y_pred=anger.predict(X_train[test])\n",
    "    scores.append(mean_squared_error(y_train[test], y_pred)) \n",
    "    score=mean_squared_error(y_train[test], y_pred)\n",
    "    k = k+1\n",
    "    y_pred=[]\n",
    "    print('Fold: %s Mean Squared Error: %.3f' % (k, score))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Joy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "joy=RandomForestRegressor()\n",
    "#joy=LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 1 Mean Squared Error: 0.039\n",
      "Fold: 2 Mean Squared Error: 0.033\n",
      "Fold: 3 Mean Squared Error: 0.035\n",
      "Fold: 4 Mean Squared Error: 0.029\n",
      "Fold: 5 Mean Squared Error: 0.037\n"
     ]
    }
   ],
   "source": [
    "data_joy=np.array(data_joy)\n",
    "target_joy=np.array(target_joy)\n",
    "#joy=LinearRegression()\n",
    "X_train, X_test, y_train, y_test = train_test_split(data_joy, target_joy, test_size=0.20, random_state=1234) \n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "# Split the data set into 'k' folds\n",
    "#kfold = StratifiedKFold(n_splits=5, random_state=1)\n",
    "kfold = KFold(n_splits=5, random_state=1)\n",
    "scores = [] \n",
    "k = 0\n",
    "\n",
    "for (train, test) in kfold.split(X_train, y_train): \n",
    "    joy.fit(X_train[train], y_train[train])          # Perform functions in pipeline\n",
    "    #score = anger.score(X_train[test], y_train[test])  # Calculate score for each fold \n",
    "    y_pred=joy.predict(X_train[test])\n",
    "    scores.append(mean_squared_error(y_train[test], y_pred)) \n",
    "    score=mean_squared_error(y_train[test], y_pred)\n",
    "    k = k+1\n",
    "    y_pred=[]\n",
    "    print('Fold: %s Mean Squared Error: %.3f' % (k, score))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 1 Mean Squared Error: 0.030\n",
      "Fold: 2 Mean Squared Error: 0.031\n",
      "Fold: 3 Mean Squared Error: 0.028\n",
      "Fold: 4 Mean Squared Error: 0.026\n",
      "Fold: 5 Mean Squared Error: 0.028\n"
     ]
    }
   ],
   "source": [
    "data_fear=np.array(data_fear)\n",
    "target_fear=np.array(target_fear)\n",
    "#fear=LinearRegression()\n",
    "fear=RandomForestRegressor()\n",
    "X_train, X_test, y_train, y_test = train_test_split(data_fear, target_fear, test_size=0.20, random_state=1234) \n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "# Split the data set into 'k' folds\n",
    "#kfold = StratifiedKFold(n_splits=5, random_state=1)\n",
    "kfold = KFold(n_splits=5, random_state=1)\n",
    "scores = [] \n",
    "k = 0\n",
    "\n",
    "for (train, test) in kfold.split(X_train, y_train): \n",
    "    fear.fit(X_train[train], y_train[train])          # Perform functions in pipeline\n",
    "    #score = anger.score(X_train[test], y_train[test])  # Calculate score for each fold \n",
    "    y_pred=fear.predict(X_train[test])\n",
    "    scores.append(mean_squared_error(y_train[test], y_pred)) \n",
    "    score=mean_squared_error(y_train[test], y_pred)\n",
    "    k = k+1\n",
    "    y_pred=[]\n",
    "    print('Fold: %s Mean Squared Error: %.3f' % (k, score))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sadness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 1 Mean Squared Error: 0.030\n",
      "Fold: 2 Mean Squared Error: 0.029\n",
      "Fold: 3 Mean Squared Error: 0.030\n",
      "Fold: 4 Mean Squared Error: 0.025\n",
      "Fold: 5 Mean Squared Error: 0.028\n"
     ]
    }
   ],
   "source": [
    "data_sadness=np.array(data_sadness)\n",
    "target_sadness=np.array(target_sadness)\n",
    "#sadness=LinearRegression()\n",
    "sadness=RandomForestRegressor()\n",
    "X_train, X_test, y_train, y_test = train_test_split(data_sadness, target_sadness, test_size=0.20, random_state=1234) \n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "# Split the data set into 'k' folds\n",
    "#kfold = StratifiedKFold(n_splits=5, random_state=1)\n",
    "kfold = KFold(n_splits=5, random_state=1)\n",
    "scores = [] \n",
    "k = 0\n",
    "\n",
    "for (train, test) in kfold.split(X_train, y_train): \n",
    "    sadness.fit(X_train[train], y_train[train])          # Perform functions in pipeline\n",
    "    #score = anger.score(X_train[test], y_train[test])  # Calculate score for each fold \n",
    "    y_pred=sadness.predict(X_train[test])\n",
    "    scores.append(mean_squared_error(y_train[test], y_pred)) \n",
    "    score=mean_squared_error(y_train[test], y_pred)\n",
    "    k = k+1\n",
    "    y_pred=[]\n",
    "    print('Fold: %s Mean Squared Error: %.3f' % (k, score))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.8137, 0.7727])"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sadness.predict(data_sadness[1:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.958"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_sadness[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def func(tweet2):\n",
    "    tweet2=[tweet2]\n",
    "    features = vectorizer.transform(tweet2)\n",
    "    features_nd = features.toarray()\n",
    "    tweet=features_nd\n",
    "    classify=model.predict(tweet)\n",
    "    if(classify=='anger'):\n",
    "        print(\"Anger with intensity-\", anger.predict(tweet))\n",
    "    if(classify=='sadness'):\n",
    "         print(\"Sadness with intensity-\", sadness.predict(tweet))\n",
    "    if(classify=='fear'):\n",
    "        print(\"Fear with intensity-\", fear.predict(tweet))\n",
    "    if(classify=='joy'):\n",
    "        print(\"Joy with intensity-\", joy.predict(tweet))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking result on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification report for valid dataset\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      anger       0.79      0.87      0.83       686\n",
      "       fear       0.86      0.78      0.82      1097\n",
      "        joy       0.87      0.91      0.89       684\n",
      "    sadness       0.82      0.82      0.82       675\n",
      "\n",
      "avg / total       0.84      0.84      0.84      3142\n",
      "\n"
     ]
    }
   ],
   "source": [
    "features = vectorizer.transform(testdata)\n",
    "features_nd = features.toarray()\n",
    "y_pred=model.predict(features_nd)\n",
    "print(\"Classification report for valid dataset\\n\",classification_report(y_pred, testdata_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anger with intensity- [0.2189]\n"
     ]
    }
   ],
   "source": [
    "func(testdata[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.14400000000000002"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testdata_int[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(\n",
    "    analyzer = 'word',\n",
    "    lowercase = False,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = vectorizer.fit_transform(\n",
    "    data\n",
    ")\n",
    "features_nd = features.toarray()\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test  = train_test_split(\n",
    "        features_nd, \n",
    "        data_labels,\n",
    "        train_size=0.80, test_size=0.20, \n",
    "        random_state=1234)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time 525.2617492372447\n",
      "CV accuracy scores: [0.8254717  0.80757098 0.81832543 0.82938389 0.8164557 ]\n",
      "CV accuracy: 0.819 +/- 0.008\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score \n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "model=RandomForestClassifier(criterion='gini',n_estimators=500,min_samples_leaf=2)\n",
    "import timeit\n",
    "tic = timeit.default_timer()\n",
    "# X_train is split into training and testing sets for each of the k folds and scores are obtained\n",
    "scores = cross_val_score(estimator=model, X=X_train, y=y_train, cv=5, n_jobs=-1)\n",
    "toc = timeit.default_timer()\n",
    "print(\"Training time\" , toc - tic)\n",
    "print('CV accuracy scores: %s' % scores) \n",
    "print('CV accuracy: %.3f +/- %.3f' % (np.mean(scores), np.std(scores))) \n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 17min 1s\n",
      "{'n_estimators': 600}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.grid_search import GridSearchCV\n",
    "param_grid = {'n_estimators': [100,200,400,500,600]}\n",
    "grid = GridSearchCV(model, param_grid,n_jobs=-1)\n",
    "%time grid.fit(X_train, y_train)\n",
    "print(grid.best_params_)\n",
    "model = grid.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification report for validation dataset\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      anger       0.94      0.77      0.84       197\n",
      "       fear       0.69      0.96      0.81       240\n",
      "        joy       0.97      0.83      0.89       191\n",
      "    sadness       0.88      0.74      0.80       164\n",
      "\n",
      "avg / total       0.86      0.83      0.84       792\n",
      "\n",
      "Testing time time 1.5385596461528621\n"
     ]
    }
   ],
   "source": [
    "tic = timeit.default_timer()\n",
    "y_fit=model.predict(X_test)\n",
    "print(\"Classification report for validation dataset\\n\",classification_report(y_test, y_fit))\n",
    "toc = timeit.default_timer()\n",
    "print(\"Testing time time\" , toc - tic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time 644.9806059257207\n",
      "CV accuracy scores: [0.8254717  0.81388013 0.80884676 0.82938389 0.81170886]\n",
      "CV accuracy: 0.818 +/- 0.008\n"
     ]
    }
   ],
   "source": [
    "tic = timeit.default_timer()\n",
    "# X_train is split into training and testing sets for each of the k folds and scores are obtained\n",
    "scores = cross_val_score(estimator=model, X=X_train, y=y_train, cv=5, n_jobs=-1)\n",
    "toc = timeit.default_timer()\n",
    "print(\"Training time\" , toc - tic)\n",
    "print('CV accuracy scores: %s' % scores) \n",
    "print('CV accuracy: %.3f +/- %.3f' % (np.mean(scores), np.std(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time 1.1365342397075437\n",
      "Classification report for validation dataset\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      anger       0.90      0.77      0.83       197\n",
      "       fear       0.66      0.96      0.78       240\n",
      "        joy       0.92      0.73      0.82       191\n",
      "    sadness       0.85      0.63      0.72       164\n",
      "\n",
      "avg / total       0.82      0.79      0.79       792\n",
      "\n",
      "Testing time time 0.4278164677307359\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "model=LogisticRegression()\n",
    "tic = timeit.default_timer()\n",
    "model.fit(X_train, y_train)\n",
    "toc = timeit.default_timer()\n",
    "print(\"Training time\" , toc - tic)\n",
    "tic = timeit.default_timer()\n",
    "y_fit=model.predict(X_test)\n",
    "print(\"Classification report for validation dataset\\n\",classification_report(y_test, y_fit))\n",
    "toc = timeit.default_timer()\n",
    "print(\"Testing time time\" , toc - tic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time 47.97415437903328\n",
      "CV accuracy scores: [0.74528302 0.72397476 0.73301738 0.75671406 0.74841772]\n",
      "CV accuracy: 0.741 +/- 0.012\n"
     ]
    }
   ],
   "source": [
    "tic = timeit.default_timer()\n",
    "# X_train is split into training and testing sets for each of the k folds and scores are obtained\n",
    "scores = cross_val_score(estimator=model, X=X_train, y=y_train, cv=5, n_jobs=-1)\n",
    "toc = timeit.default_timer()\n",
    "print(\"Training time\" , toc - tic)\n",
    "print('CV accuracy scores: %s' % scores) \n",
    "print('CV accuracy: %.3f +/- %.3f' % (np.mean(scores), np.std(scores))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1min 4s\n",
      "{'C': 5, 'penalty': 'l1'}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.grid_search import GridSearchCV\n",
    "param_grid = {'penalty': ['l1','l2'],\n",
    "             'C':[1,5,10]}\n",
    "grid = GridSearchCV(model, param_grid,n_jobs=-1)\n",
    "%time grid.fit(X_train, y_train)\n",
    "print(grid.best_params_)\n",
    "model = grid.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification report for validation dataset\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      anger       0.91      0.79      0.85       197\n",
      "       fear       0.76      0.94      0.84       240\n",
      "        joy       0.96      0.86      0.91       191\n",
      "    sadness       0.88      0.83      0.86       164\n",
      "\n",
      "avg / total       0.87      0.86      0.86       792\n",
      "\n",
      "Testing time time 0.7451992886708467\n"
     ]
    }
   ],
   "source": [
    "tic = timeit.default_timer()\n",
    "y_fit=model.predict(X_test)\n",
    "print(\"Classification report for validation dataset\\n\",classification_report(y_test, y_fit))\n",
    "toc = timeit.default_timer()\n",
    "print(\"Testing time time\" , toc - tic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time 52.084890534923034\n",
      "CV accuracy scores: [0.86477987 0.84700315 0.83886256 0.84992101 0.83386076]\n",
      "CV accuracy: 0.847 +/- 0.011\n"
     ]
    }
   ],
   "source": [
    "tic = timeit.default_timer()\n",
    "# X_train is split into training and testing sets for each of the k folds and scores are obtained\n",
    "scores = cross_val_score(estimator=model, X=X_train, y=y_train, cv=5, n_jobs=-1)\n",
    "toc = timeit.default_timer()\n",
    "print(\"Training time\" , toc - tic)\n",
    "print('CV accuracy scores: %s' % scores) \n",
    "print('CV accuracy: %.3f +/- %.3f' % (np.mean(scores), np.std(scores))) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time 276.1597939480889\n",
      "Classification report for validation dataset\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      anger       0.89      0.83      0.86       197\n",
      "       fear       0.79      0.95      0.86       240\n",
      "        joy       0.94      0.84      0.89       191\n",
      "    sadness       0.85      0.77      0.81       164\n",
      "\n",
      "avg / total       0.86      0.85      0.85       792\n",
      "\n",
      "Testing time time 43.27931745304886\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "model=SVC(kernel='linear')\n",
    "tic = timeit.default_timer()\n",
    "model.fit(X_train, y_train)\n",
    "toc = timeit.default_timer()\n",
    "print(\"Training time\" , toc - tic)\n",
    "tic = timeit.default_timer()\n",
    "y_fit=model.predict(X_test)\n",
    "print(\"Classification report for validation dataset\\n\",classification_report(y_test, y_fit))\n",
    "toc = timeit.default_timer()\n",
    "print(\"Testing time time\" , toc - tic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time 600.4453322846348\n",
      "CV accuracy scores: [0.83490566 0.78391167 0.82464455 0.81516588 0.8085443 ]\n",
      "CV accuracy: 0.813 +/- 0.017\n"
     ]
    }
   ],
   "source": [
    "tic = timeit.default_timer()\n",
    "# X_train is split into training and testing sets for each of the k folds and scores are obtained\n",
    "scores = cross_val_score(estimator=model, X=X_train, y=y_train, cv=5, n_jobs=-1)\n",
    "toc = timeit.default_timer()\n",
    "print(\"Training time\" , toc - tic)\n",
    "print('CV accuracy scores: %s' % scores) \n",
    "print('CV accuracy: %.3f +/- %.3f' % (np.mean(scores), np.std(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intensity calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 1 Mean Squared Error: 0.027\n",
      "Fold: 2 Mean Squared Error: 0.026\n",
      "Fold: 3 Mean Squared Error: 0.022\n",
      "Fold: 4 Mean Squared Error: 0.020\n",
      "Fold: 5 Mean Squared Error: 0.022\n"
     ]
    }
   ],
   "source": [
    "anger=RandomForestRegressor()\n",
    "X_train, X_test, y_train, y_test = train_test_split(data_anger, target_anger, test_size=0.20, random_state=1234) \n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "# Split the data set into 'k' folds\n",
    "#kfold = StratifiedKFold(n_splits=5, random_state=1)\n",
    "kfold = KFold(n_splits=5, random_state=1)\n",
    "scores = [] \n",
    "k = 0\n",
    "\n",
    "for (train, test) in kfold.split(X_train, y_train): \n",
    "    anger.fit(X_train[train], y_train[train])          # Perform functions in pipeline\n",
    "    score = anger.score(X_train[test], y_train[test])  # Calculate score for each fold \n",
    "    y_pred=anger.predict(X_train[test])\n",
    "    scores.append(mean_squared_error(y_train[test], y_pred)) \n",
    "    score=mean_squared_error(y_train[test], y_pred)\n",
    "    k = k+1\n",
    "    y_pred=[]\n",
    "    print('Fold: %s Mean Squared Error: %.3f' % (k, score))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 1 Mean Squared Error: 0.043\n",
      "Fold: 2 Mean Squared Error: 0.035\n",
      "Fold: 3 Mean Squared Error: 0.035\n",
      "Fold: 4 Mean Squared Error: 0.032\n",
      "Fold: 5 Mean Squared Error: 0.039\n"
     ]
    }
   ],
   "source": [
    "joy=LinearRegression()\n",
    "X_train, X_test, y_train, y_test = train_test_split(data_joy, target_joy, test_size=0.20, random_state=1234) \n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "# Split the data set into 'k' folds\n",
    "#kfold = StratifiedKFold(n_splits=5, random_state=1)\n",
    "kfold = KFold(n_splits=5, random_state=1)\n",
    "scores = [] \n",
    "k = 0\n",
    "\n",
    "for (train, test) in kfold.split(X_train, y_train): \n",
    "    anger.fit(X_train[train], y_train[train])          # Perform functions in pipeline\n",
    "    #score = anger.score(X_train[test], y_train[test])  # Calculate score for each fold \n",
    "    y_pred=anger.predict(X_train[test])\n",
    "    scores.append(mean_squared_error(y_train[test], y_pred)) \n",
    "    score=mean_squared_error(y_train[test], y_pred)\n",
    "    k = k+1\n",
    "    y_pred=[]\n",
    "    print('Fold: %s Mean Squared Error: %.3f' % (k, score))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 1 Mean Squared Error: 0.032\n",
      "Fold: 2 Mean Squared Error: 0.031\n",
      "Fold: 3 Mean Squared Error: 0.029\n",
      "Fold: 4 Mean Squared Error: 0.027\n",
      "Fold: 5 Mean Squared Error: 0.026\n"
     ]
    }
   ],
   "source": [
    "fear=LinearRegression()\n",
    "X_train, X_test, y_train, y_test = train_test_split(data_fear, target_fear, test_size=0.20, random_state=1234) \n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "# Split the data set into 'k' folds\n",
    "#kfold = StratifiedKFold(n_splits=5, random_state=1)\n",
    "kfold = KFold(n_splits=5, random_state=1)\n",
    "scores = [] \n",
    "k = 0\n",
    "\n",
    "for (train, test) in kfold.split(X_train, y_train): \n",
    "    anger.fit(X_train[train], y_train[train])          # Perform functions in pipeline\n",
    "    #score = anger.score(X_train[test], y_train[test])  # Calculate score for each fold \n",
    "    y_pred=anger.predict(X_train[test])\n",
    "    scores.append(mean_squared_error(y_train[test], y_pred)) \n",
    "    score=mean_squared_error(y_train[test], y_pred)\n",
    "    k = k+1\n",
    "    y_pred=[]\n",
    "    print('Fold: %s Mean Squared Error: %.3f' % (k, score))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 1 Mean Squared Error: 0.028\n",
      "Fold: 2 Mean Squared Error: 0.031\n",
      "Fold: 3 Mean Squared Error: 0.030\n",
      "Fold: 4 Mean Squared Error: 0.026\n",
      "Fold: 5 Mean Squared Error: 0.027\n"
     ]
    }
   ],
   "source": [
    "sadness=LinearRegression()\n",
    "X_train, X_test, y_train, y_test = train_test_split(data_sadness, target_sadness, test_size=0.20, random_state=1234) \n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "# Split the data set into 'k' folds\n",
    "#kfold = StratifiedKFold(n_splits=5, random_state=1)\n",
    "kfold = KFold(n_splits=5, random_state=1)\n",
    "scores = [] \n",
    "k = 0\n",
    "\n",
    "for (train, test) in kfold.split(X_train, y_train): \n",
    "    anger.fit(X_train[train], y_train[train])          # Perform functions in pipeline\n",
    "    #score = anger.score(X_train[test], y_train[test])  # Calculate score for each fold \n",
    "    y_pred=anger.predict(X_train[test])\n",
    "    scores.append(mean_squared_error(y_train[test], y_pred)) \n",
    "    score=mean_squared_error(y_train[test], y_pred)\n",
    "    k = k+1\n",
    "    y_pred=[]\n",
    "    print('Fold: %s Mean Squared Error: %.3f' % (k, score))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification report for valid dataset\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      anger       0.79      0.89      0.84       671\n",
      "       fear       0.88      0.78      0.83      1118\n",
      "        joy       0.88      0.93      0.90       675\n",
      "    sadness       0.83      0.82      0.83       678\n",
      "\n",
      "avg / total       0.85      0.85      0.85      3142\n",
      "\n"
     ]
    }
   ],
   "source": [
    "features = vectorizer.transform(testdata)\n",
    "features_nd = features.toarray()\n",
    "y_pred=model.predict(features_nd)\n",
    "print(\"Classification report for valid dataset\\n\",classification_report(y_pred, testdata_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We can clearly see that tfidfvectorizer is performing better than count vectorizer. Tf idf is different from countvectorizer. Countvectorizer gives equal weightage to all the words, i.e. a word is converted to a column (in a dataframe for example) and for each document, it is equal to 1 if it is present in that doc else 0.  Apart from giving this information, tfidf says how important that word is to that document with respect to the corpus. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We can observe that logistic regression takes the least time for traing and testing the dataset for both the vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
