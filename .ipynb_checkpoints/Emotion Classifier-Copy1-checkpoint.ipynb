{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Emotion Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset set contained in four text files consists of tweets for four different emotions: anger, fear, joy and sadness.<br>\n",
    "Along with the tweet, the intensity or degree of emotion X felt by the speaker (a real-valued score between 0 and 1) is also provided. <br>\n",
    "The maximum possible score 1 stands for feeling the maximum amount of emotion X (or having a mental state maximally inclined towards feeling emotion X). The minimum possible score 0 stands for feeling the least amount of emotion X (or having a mental state maximally away from feeling emotion X). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Installing required package:<br>\n",
    "```\n",
    "pip3 install nltk\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reading the tweets and their corresponding emotion and intensity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import DataFrame\n",
    "import pandas as pd\n",
    "\n",
    "data = [] # Tweets\n",
    "data_labels = [] # Emotion label (anger, fear, joy, or sadness)\n",
    "data_int = [] # Intensityy of each emotion\n",
    "\n",
    "dataset=pd.read_csv(\"anger-ratings-0to1.train.txt\",delimiter=\"\\t\",names=['id','tweet','emotion','intensity'])\n",
    "for i in range(len(dataset)):\n",
    "    data.append(dataset.iat[i,1])\n",
    "    data_labels.append('anger')\n",
    "    data_int.append(dataset.iat[i,3])\n",
    "    \n",
    "dataset=pd.read_csv(\"fear-ratings-0to1.train.txt\",delimiter=\"\\t\",names=['id','tweet','emotion','intensity'])\n",
    "for i in range(len(dataset)):\n",
    "    data.append(dataset.iat[i,1])\n",
    "    data_labels.append('fear')\n",
    "    data_int.append(dataset.iat[i,3])\n",
    "\n",
    "dataset=pd.read_csv(\"joy-ratings-0to1.train.txt\",delimiter=\"\\t\",names=['id','tweet','emotion','intensity'])\n",
    "for i in range(len(dataset)):\n",
    "    data.append(dataset.iat[i,1])\n",
    "    data_labels.append('joy')\n",
    "    data_int.append(dataset.iat[i,3])\n",
    "\n",
    "dataset=pd.read_csv(\"sadness-ratings-0to1.train.txt\",delimiter=\"\\t\",names=['id','tweet','emotion','intensity'])\n",
    "for i in range(len(dataset)):\n",
    "    data.append(dataset.iat[i,1])\n",
    "    data_labels.append('sadness')\n",
    "    data_int.append(dataset.iat[i,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>tweet</th>\n",
       "      <th>emotion</th>\n",
       "      <th>intensity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>40000</td>\n",
       "      <td>Depression sucks! #depression</td>\n",
       "      <td>sadness</td>\n",
       "      <td>0.958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>40001</td>\n",
       "      <td>Feeling worthless as always #depression</td>\n",
       "      <td>sadness</td>\n",
       "      <td>0.958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>40002</td>\n",
       "      <td>Feeling worthless as always</td>\n",
       "      <td>sadness</td>\n",
       "      <td>0.958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>40003</td>\n",
       "      <td>My #Fibromyalgia has been really bad lately which is not good for my mental state. I feel very overwhelmed #anxiety #bipolar #depression</td>\n",
       "      <td>sadness</td>\n",
       "      <td>0.946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>40004</td>\n",
       "      <td>Im think ima lay in bed all day and sulk. Life is hitting me to hard rn</td>\n",
       "      <td>sadness</td>\n",
       "      <td>0.934</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id  \\\n",
       "0  40000   \n",
       "1  40001   \n",
       "2  40002   \n",
       "3  40003   \n",
       "4  40004   \n",
       "\n",
       "                                                                                                                                      tweet  \\\n",
       "0  Depression sucks! #depression                                                                                                              \n",
       "1  Feeling worthless as always #depression                                                                                                    \n",
       "2  Feeling worthless as always                                                                                                                \n",
       "3  My #Fibromyalgia has been really bad lately which is not good for my mental state. I feel very overwhelmed #anxiety #bipolar #depression   \n",
       "4  Im think ima lay in bed all day and sulk. Life is hitting me to hard rn                                                                    \n",
       "\n",
       "   emotion  intensity  \n",
       "0  sadness  0.958      \n",
       "1  sadness  0.958      \n",
       "2  sadness  0.958      \n",
       "3  sadness  0.946      \n",
       "4  sadness  0.934      "
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display first few examples\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffling the data\n",
    "from random import shuffle\n",
    "dv = []\n",
    "dl = []\n",
    "di = []\n",
    "index_shuf = list(range(len(data)))\n",
    "shuffle(index_shuf)\n",
    "for i in index_shuf:\n",
    "    dv.append(data[i])\n",
    "    dl.append(data_labels[i])\n",
    "    di.append(data_int[i])\n",
    "data = dv\n",
    "data_labels = dl\n",
    "data_int = di"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature extraction using CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer    \n",
    "\n",
    "vectorizer = CountVectorizer(\n",
    "    analyzer = 'word',\n",
    "    lowercase = False,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### An example using CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['this is great', 'This is too great to be great', 'THIS IS GREAT!']\n"
     ]
    }
   ],
   "source": [
    "example = ['this is great','This is too great to be great','THIS IS GREAT!']\n",
    "print(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['GREAT', 'IS', 'THIS', 'This', 'be', 'great', 'is', 'this', 'to', 'too']"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_eg = vectorizer.fit_transform(\n",
    "    example\n",
    ")\n",
    "features_nd_eg = features_eg.toarray() # for easy usage\n",
    "vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extracting features from tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = vectorizer.fit_transform(\n",
    "    data\n",
    ")\n",
    "features_nd = features.toarray() # for easy usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test  = train_test_split(\n",
    "        features_nd, \n",
    "        data_labels,\n",
    "        train_size=0.80, test_size=0.20, \n",
    "        random_state=1234)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "log_model = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_model = log_model.fit(X=X_train, y=y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = log_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8492392807745505"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.mean(y_pred==y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "joy : #BridgetJonesBaby is the best thing I've seen in ages! So funny, I've missed Bridget! #love  #TeamMark\n",
      "joy : @Zerfash â€” can't wait.' She said cheerfully and grinned.\n",
      "fear : I don't know how people can binge watch horror films ...ALONE!ðŸ˜“ðŸ˜°\n",
      "sadness : Carry on my wayward son, there'll be peace when you are done. Lay your weary head to rest. Don't you cry no more. #Supernatural\n",
      "sadness : If a friend lost his/her phone, how long do they have to mourn their lost phones before you ask for their earpiece?\n",
      "fear : Y'all really insult coz of soccer???  Lmao, wow!!!!!!\n",
      "fear : I want to slide into the dms but im too fucking shy\n"
     ]
    }
   ],
   "source": [
    "# Printing the predictions for some random test data\n",
    "import random\n",
    "\n",
    "j = random.randint(0,len(X_test)-7)\n",
    "for i in range(j,j+7):\n",
    "    ind = features_nd.tolist().index(X_test[i].tolist())\n",
    "    print(y_pred[i],\":\",data[ind].strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8492392807745505\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "print(accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "```\n",
    "There are two sets each containing 4 files for each emotion provided for training and development. \n",
    "Combine these two sets for training and use 5-fold cross-validation \n",
    "to find out the Accuracy in all the cases mentioned below.\n",
    "```\n",
    "\n",
    "1. Calculate the accuracy using Random Forest Classifier and tune the number of estimators to get the best results. Comment on the same.\n",
    "2. Now use Logistic Regression and observe the accuracy value. Can the performance be further improved by using L1 and L2 regularizations?\n",
    "3. Repeat the same using Support Vector Classifier.\n",
    "4. Estimate the training & testing time for each classifier and comment on the results.\n",
    "5. Fit different regression models for each emotion and display mean square error for test set.\n",
    "6. A separate test set is provided. Use one of the classification models implemented earlier to determine the corresponding emotion for each tweet in this set. Use the linear regression models to calculate the emotional intensity.\n",
    "\n",
    "```In all the above cases, CountVectorizer can be used for feature extraction```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
